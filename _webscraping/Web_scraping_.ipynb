{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping rules\n",
    "- You should check a site's terms and conditions before you scrape them. It's their data and they likely have some rules to govern it.\n",
    "- Be nice - A computer will send web requests much quicker than a user can. Make sure you space out your requests a bit so that you don't hammer the site's server.\n",
    "- Scrapers break - Sites change their layout all the time. If that happens, be prepared to rewrite your code.\n",
    "- Web pages are inconsistent - There's sometimes some manual clean up that has to happen even after you've gotten your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import necessary modules</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## requests\n",
    "- requests executes HTTP requests, like GET\n",
    "- The requests object holds the results of the request. This is page content and other items like HTTP status codes and headers.\n",
    "- requests only gets the page content without any parsing.\n",
    "- Beautiful Soup does the parsing of the HTML and finding content within the HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.epicurious.com/search/Tofu Chili\"\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    print(\"Success\")\n",
    "else:\n",
    "    print(\"Failure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requests - connect as function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        print('successfully connected, response code: {}'.format(response.status_code))\n",
    "    else:\n",
    "        print('connection failed')\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connect(url);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requests - passing cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "# You may pass in custom cookie\n",
    "r = session.get('http://httpbin.org/get', cookies={'my-cookie': 'browser'})\n",
    "print(r.text)\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requests - streaming\n",
    "- http://docs.python-requests.org/en/master/user/advanced/#streaming-requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('http://httpbin.org/stream/20', stream=True)\n",
    "for line in r.iter_lines():\n",
    " # filter out keep-alive new lines\n",
    " if line:\n",
    "    decoded_line = line.decode('utf-8')\n",
    "    print(json.loads(decoded_line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requests pass search keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = input(\"Please enter the things you want to see in a recipe: \")\n",
    "connect('http://www.epicurious.com/search/' + keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chars = 1000\n",
    "soup = BeautifulSoup(connect(url).content, 'lxml')\n",
    "print(soup.prettify()[:n_chars])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get result page as function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_page(url, keywords=''):\n",
    "    response = requests.get(url + keywords)\n",
    "    if not response.status_code == 200:\n",
    "        return None\n",
    "    return BeautifulSoup(response.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = input(\"Please enter the things you want to see in a recipe: \")\n",
    "url = 'http://www.epicurious.com/search/'\n",
    "results_page = result_page(url, keywords)\n",
    "# print(results_page.prettify()[100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Headless Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://duo.com/decipher/driving-headless-chrome-with-python\n",
    "# Install Chrome Canary\n",
    "# Download latest Chromedriver (to Downloads)\n",
    "# !mkdir going_headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp ~/Downloads/chromedriver going_headless/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls going_headless/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options  \n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By \n",
    "# from selenium.webdriver.support.ui import WebDriverWait \n",
    "# from selenium.webdriver.support import expected_conditions as EC \n",
    "# from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = Options()  \n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--incognito\")\n",
    "chrome_options.binary_location = '/Applications/Google Chrome Canary.app/Contents/MacOS/Google Chrome Canary'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selenium interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(executable_path=os.path.abspath('going_headless/chromedriver'), chrome_options=chrome_options)  \n",
    "driver.get(\"http://www.duo.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General driver functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(driver.name, driver.title, driver.current_url, driver.get_cookies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other driver functions\n",
    "driver.back()\n",
    "driver.forward()\n",
    "driver.refresh()\n",
    "driver.quit()\n",
    "# driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactive functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# magnifying_glass = driver.find_element_by_id(\"js-open-icon\")  \n",
    "# if magnifying_glass.is_displayed():  \n",
    "#     magnifying_glass.click()  \n",
    "# else:  \n",
    "#     menu_button = driver.find_element_by_css_selector(\".menu-trigger.local\")  \n",
    "#     menu_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_field = driver.find_element_by_id(\"site-search\")  \n",
    "# search_field.clear()  \n",
    "# search_field.send_keys(\"Olabode\")  \n",
    "# search_field.send_keys(Keys.RETURN)  \n",
    "# assert \"Looking Back at Android Security in 2016\" in driver.page_source   \n",
    "# driver.page_source[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Browser Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url ='https://www.zara.com/uk/en/search?searchTerm='\n",
    "keyword = 'dress'\n",
    "url += keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# menu_man = driver.find_element_by_partial_link_text('MAN').click()\n",
    "url ='https://www.zara.com/uk/en/search?searchTerm='\n",
    "keyword = 'man trousers'\n",
    "url += keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser_options = Options()  \n",
    "browser_options.add_argument(\"--incognito\")\n",
    "browser_options.binary_location = '/Applications/Google Chrome Canary.app/Contents/MacOS/Google Chrome Canary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver.close()\n",
    "driver.quit()\n",
    "browser.quit()\n",
    "# browser.close()\n",
    "browser = webdriver.Chrome(executable_path=os.path.abspath('going_headless/chromedriver'), chrome_options=browser_options)  \n",
    "browser.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# //*[@id=\"catalog-area\"]/img\n",
    "# //*[@id=\"catalog-area\"]/script[1]/text()\n",
    "# //*[@id=\"catalog-area\"]/img\n",
    "\n",
    "# # country popup\n",
    "# <div class=\"popup _popup\" role=\"dialog\" id=\"geolocation-popup\" style=\"z-index: 500; left: 219px; top: 193px; opacity: 1;\"><div class=\"_popup-wrapper\"><div class=\"popup-header \"><h1 class=\"popup-title \" data-first=\"true\" tabindex=\"0\" aria-label=\".\"></h1><div class=\"close  _closeHandler\" role=\"button\" aria-label=\"close\" tabindex=\"0\"><i class=\"icon icon-close\"></i></div></div><div class=\"popup-navigation\"><div class=\"componentNav no-display _componentNav\"><div class=\"prevItem _prevItem\"><span class=\"arrow left-arrow\"></span></div><div class=\"nextItem _nextItem\"><span class=\"arrow right-arrow\"></span></div></div></div><div class=\"popup-content\" style=\"height: auto; min-height: 0px;\"><div class=\"content _content\"><header><h2 class=\"popup-title\">Hello,</h2></header><section class=\"\" data-controller=\"shared/geolocation-main-controller\"><div class=\"info\">You are accessing this website from Nederland / Netherlands. Would you like to visit our website in Nederland / Netherlands?</div><section class=\"button-grp\"><button class=\"button-primary button-big _confirm\">Yes, go to the website for Nederland / Netherlands</button><button class=\"button-secondary button-big _closeHandler\">No, continue on the website for United Kingdom</button></section><div class=\"footer\"><span>We are present in over 50 stores, please select yours <a class=\"_country-selector-trigger\">here</a></span></div></section></div></div><div class=\"popup-footer\"><div data-last=\"true\" tabindex=\"0\" aria-lable=\".\"><div></div></div></div></div><div id=\"loading\" style=\"position: absolute; display: none;\"></div></div>\n",
    "# .popup .popup-header .close\n",
    "# #geolocation-popup > div._popup-wrapper > div.popup-header > div\n",
    "# #geolocation-popup > div._popup-wrapper > div.popup-content > div > section > section > button.button-secondary.button-big._closeHandler\n",
    "\n",
    "# var r = Math.random(),a = r * 10000000000000,i = document.createElement(\"IMG\");i.src = \"https://ad.doubleclick.net/ddm/activity/src=8370426;type=counter;cat=remar0;dc_lat=;dc_rdid=;tag_for_child_directed_treatment=;ord=\" + a;document.body.appendChild(i);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #products > ul\n",
    "# //*[@id=\"products\"]/ul\n",
    "# //*[@id=\"product-img-5338551\"]\n",
    "# //*[@id=\"product-5338551\"]/a/div\n",
    "# #product-5338551 > a > div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "SCROLL_PAUSE_TIME = 6\n",
    "# SCROLL_HEIGHT = 'document.body.scrollHeight'\n",
    "df = pd.DataFrame()\n",
    "for i, tag in enumerate(browser.find_elements_by_xpath('//*[@id=\"products\"]/ul/li/a/div/img')):\n",
    "    # Scroll every 4 items\n",
    "    if i % 4 == 0:\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "        if i > 40:\n",
    "            break\n",
    "    id_ = tag.get_attribute('id')\n",
    "    alt = tag.get_attribute('alt')\n",
    "    src = tag.get_attribute('src')\n",
    "    df.loc[i, 'id'] = id_.split('-')[2]\n",
    "    df.loc[i, 'ts'] = src.split('=')[1]\n",
    "    df.loc[i, 'description'] = alt\n",
    "    df.loc[i, 'source'] = src\n",
    "    \n",
    "    print('\\b\\b\\b{}'.format(i), sep='', end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()\n",
    "driver = webdriver.Chrome(executable_path=os.path.abspath('going_headless/chromedriver'), chrome_options=chrome_options) \n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.current_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# menu_man = driver.find_element_by_partial_link_text('MAN').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "SCROLL_PAUSE_TIME = 2\n",
    "\n",
    "for image in soup.find_all('img'):\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    print(image['src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in soup.find_all('li'):\n",
    "    try:\n",
    "        image = tag.a.img\n",
    "        if tag.a.text[:3] == 'MAN':\n",
    "            print(tag.a.text)\n",
    "            tag.a.click()\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "styles = {}\n",
    "for i, tag in enumerate(soup.find_all('img')):\n",
    "    try:\n",
    "        src = tag['src']\n",
    "        if src[:8] == '//static':\n",
    "            key = '_'.join(tag['alt'].split())\n",
    "            key += '_{}'.format(tag['data-productid'])\n",
    "            styles[key] = 'https:{}'.format(tag['src'], i)\n",
    "        else:\n",
    "            print(src)\n",
    "    except:\n",
    "        continue\n",
    "styles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in styles:\n",
    "    r = requests.get(styles[key], allow_redirects=True)\n",
    "    open('images/{}.jpg'.format(key), 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Take screenshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get_screenshot_as_file('sample_screenshot_2.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### use search field interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_field = driver.find_element_by_class_name(\"search\")  \n",
    "search_field\n",
    "# search_field.clear()\n",
    "# search_field.send_keys(keyword)\n",
    "# search_field.send_keys(Keys.RETURN)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for li in driver.find_elements_by_tag_name('img')[2:25]:\n",
    "    src = li.get_attribute('src')\n",
    "    alt = li.get_attribute('alt')\n",
    "    print(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in driver.find_elements_by_tag_name('img')[22:42]:\n",
    "    src = tag.get_attribute('src')\n",
    "    alt = tag.get_attribute('alt')\n",
    "    print(src, alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "driver.implicitly_wait(20) # seconds\n",
    "    \n",
    "for tag in driver.find_elements_by_tag_name('img')[:250]:\n",
    "    src = tag.get_attribute('src')\n",
    "    alt = tag.get_attribute('alt')\n",
    "    print(src, alt)\n",
    "    try:\n",
    "        alert = driver.switch_to_alert()\n",
    "        alert.dismiss()\n",
    "        alert.click()\n",
    "#         alert.accept()\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Chrome instance with Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath = \"//*[@class='product-list _productList']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = driver.find_elements_by_xpath(xpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events1 = driver.find_elements_by_xpath('//ul[contains(., \"product\")]/*') # //*[@id=\"product-6504767\"]\n",
    "events2 = driver.find_elements_by_xpath('//*[@id=\"products\"]/*')\n",
    "events1, events2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in events:\n",
    "    print(event.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Python events TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_events(url):\n",
    "    driver = webdriver.Chrome(executable_path=os.path.abspath('going_headless/chromedriver'), chrome_options=chrome_options) \n",
    "    driver.get(url)\n",
    "    events = driver.find_elements_by_xpath('//ul[contains(., \"product-list _productList\")]/li')\n",
    "    print(events)\n",
    "    for event in events:\n",
    "        print(event)\n",
    "        event_details = dict()\n",
    "        event_details['name'] = event.find_element_by_xpath('h3[@class=\"event-title\"]/a').text\n",
    "        event_details['location'] = event.find_element_by_xpath('p/span[@class=\"event-location\"]').text\n",
    "        event_details['time'] = event.find_element_by_xpath('p/time').text\n",
    "        print(event_details)\n",
    "    driver.close()                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = get_events('https://www.python.org/events/python-events/')\n",
    "print(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### New section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = html.fromstring(str(results_page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[div for div in tree.xpath(\"/html/body/div\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! Index starts @ 1 not 0\n",
    "[etree.tostring(node)[:100] for node in tree.xpath(\"/html/body/div\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[etree.tostring(node)[:100] for node in tree.xpath(\"//div\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['class:{}, id:{}'.format(node.xpath(\"@class\"), node.xpath(\"@id\")) for node in tree.xpath(\"//div\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['class:{}, id:{}'.format(node.xpath(\"@class\"), node.xpath(\"@id\")) for node in tree.xpath(\"//section\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[etree.tostring(node) for node in tree.xpath(\"//*[@id='products']/*\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[etree.tostring(node) for node in tree.xpath(\"//ul[@class='product-list _productList']/*\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[etree.tostring(node)[:100] for node in tree.xpath(\"//*[contains(., 'dress')]\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['class:{}, id:{}'.format(node.xpath(\"@class\"), node.xpath(\"@name\")) for node in tree.xpath(\"//*[contains(., 'product')]\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[etree.tostring(div)[:100] for div in tree.xpath(\"//section[@class='_results']\")] # product-list _productList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[etree.tostring(div)[:100] for div in tree.xpath(\"//section._results\")] # product-list _productList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[etree.tostring(div)[:100] for div in tree.xpath(\"//lu[@class='product-list _productList']\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[etree.tostring(div)[:100] for div in tree.xpath(\"//*[@class='product _product']\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.xpath(\"//*[@id='product-6504767']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree.xpath(\"/html/body/div[2]/section\")\n",
    "product_list = [etree.tostring(li) for li in tree.xpath(\"/html/body/div[2]/section/div/section/ul/li\")]\n",
    "product_list\n",
    "# tree.xpath('//*[@id=\"product-6504767\"')\n",
    "tree.xpath('//div[@class=\"product-info _product-info\"]') # //a[@class=\"item _item\"]/@href')  # class=\"_ariaResults wai-aria-messages\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[li for li in product_list]\n",
    "# etree.tostring(tree.xpath(\"/html/body/div[2]/section/div/section/ul\"))\n",
    "# etree.tostring(product_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>BS4 functions</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### find_all  \n",
    "list of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_page = result_page(url, keyword)\n",
    "results_page.body.div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lines = 5\n",
    "all_a_tags = results_page.find_all('a')\n",
    "print(type(all_a_tags))\n",
    "all_a_tags[:n_lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### find  \n",
    "first result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_tag = results_page.find('div')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_page.find_all('a')[0] == results_page.find('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(div_tag), div_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursively apply on elements (traverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(results_page\n",
    "    .find('div')\n",
    "    .find('a')\n",
    "    .get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find and find_all  \n",
    "as css selectors\n",
    "<li>using selector=value, e.g. class_='recipe-content-card')\n",
    "<li>using a dictionary, e.g. {'class':'recipe-content-card'}\n",
    "<li>class is a reserved word in python, please use as 'class' or class_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = 'recipe-content-card'\n",
    "results_page.find_all('article', class_=selector)[0] == results_page.find('article', {'class':selector})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_text() \n",
    "Returns the content enclosed in a tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_page.find('article',{'class':selector}).get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get()\n",
    "Returns the value of a tag attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_tag = results_page.find('article',{'class':selector})\n",
    "recipe_link = recipe_tag.find('a')\n",
    "recipe_content = recipe_tag.find('a').get_text()\n",
    "link_url = recipe_link.get('href')\n",
    "\n",
    "print('a tag: {}\\n - content: {}\\n - link url: {}\\n - link type: {} '.format(recipe_link, recipe_content, link_url, type(link_url)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recipes(url, keywords='', selector=''):\n",
    "    recipe_list = []\n",
    "    try:\n",
    "        results_page = result_page(url, keywords)\n",
    "        recipes = results_page.find_all('article', class_=selector)\n",
    "        \n",
    "        for recipe in recipes:\n",
    "            recipe_link = url + recipe.find('a').get('href')\n",
    "            recipe_name = recipe.find('a').get_text()\n",
    "            try:\n",
    "                recipe_description = recipe.find('p', class_='dek').get_text()\n",
    "            except:\n",
    "                recipe_description = ''\n",
    "            recipe_list.append((recipe_name, recipe_link, recipe_description))\n",
    "            \n",
    "        return recipe_list\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.epicurious.com/search/'\n",
    "keywords = input('Please enter the things you want to see in a recipe: ')\n",
    "selector = 'recipe-content-card'\n",
    "get_recipes(url, keywords, selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recipe ingredients and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recipe_info(url, keywords='', selector=''):\n",
    "    recipe_dict = {}\n",
    "    try:\n",
    "        results_page = result_page(url, keywords)\n",
    "        ingredient_list, prep_steps_list = [], []\n",
    "        for ingredient in results_page.find_all('li', class_='ingredient'):\n",
    "            ingredient_list.append(ingredient.get_text())\n",
    "            \n",
    "        for prep_step in results_page.find_all('li', class_='preparation-step'):\n",
    "            prep_steps_list.append(prep_step.get_text().strip())\n",
    "            \n",
    "        recipe_dict['ingredients'], recipe_dict['preparation'] = ingredient_list, prep_steps_list\n",
    "        return recipe_dict\n",
    "    except:\n",
    "        return recipe_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.epicurious.com'\n",
    "link = '/recipes/food/views/spicy-lemongrass-tofu-233844'\n",
    "recipe_info = get_recipe_info(url + link)\n",
    "recipe_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_recipes(url, keywords='', selector=''):\n",
    "    results = []\n",
    "    all_recipes = get_recipes(url, keywords, selector)\n",
    "    for recipe in all_recipes:\n",
    "        recipe_dict = get_recipe_info(recipe[1])\n",
    "        recipe_dict['name'] = recipe[0]\n",
    "        recipe_dict['description'] = recipe[2]\n",
    "        results.append(recipe_dict)\n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.epicurious.com/search/'\n",
    "keywords = input('Please enter the things you want to see in a recipe: ')\n",
    "selector = 'recipe-content-card'\n",
    "all_recipes = get_all_recipes(url, keywords, selector)\n",
    "all_recipes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(all_recipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data_string = '[{\"b\": [2, 4], \"c\": 3.0, \"a\": \"A\"}]'\n",
    "python_data = json.loads(data_string)\n",
    "print(python_data)\n",
    "print(python_data[0]['b'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### json.loads recursively decodes a string in JSON format into equivalent python objects\n",
    " - data_string's outermost element is converted into a python list\n",
    " - the first element of that list is converted into a dictionary\n",
    " - the key of that dictionary is converted into a string\n",
    " - the value of that dictionary is converted into a list of two integer elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(data_string), type(python_data))\n",
    "print(type(python_data[0]), python_data[0])\n",
    "print(type(python_data[0]['b']), python_data[0]['b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### json.dumps and json.loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON_string = \"JSON throws exception when not in correct format\"\n",
    "print(JSON_string)\n",
    "\n",
    "# Stringify strings\n",
    "JSON_stringified = json.dumps(JSON_string)\n",
    "print(JSON_stringified)\n",
    "\n",
    "# Correct\n",
    "json.loads(JSON_stringified)\n",
    "\n",
    "# JSONDecodeError \n",
    "# json.loads(JSON_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = 'Amsterdam, Netherlands'\n",
    "url = 'https://maps.googleapis.com/maps/api/geocode/json?address={}'.format(address)\n",
    "response = requests.get(url).json()\n",
    "type(response), response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'http://maps.googleapis.com/maps/api/geocode/json'\n",
    "my_params = {'address': '100 Broadway, New York, NY, U.S.A', \n",
    "             'language': 'en'}\n",
    "response = requests.get(base_url, params=my_params)\n",
    "results = response.json()['results']\n",
    "x_geo = results[0]['geometry']['location']\n",
    "print(x_geo['lng'], x_geo['lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get JSON formatted content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(url, decode='utf-8'):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if not response.status_code == 200:\n",
    "            print('HTTP error, response code: {}'.format(response.status_code))\n",
    "        else:\n",
    "            try:\n",
    "                response_data = response.json()\n",
    "            except:\n",
    "                print(\"response not in valid JSON format\")\n",
    "    except:\n",
    "        print('something went wrong with requests.get')\n",
    "        \n",
    "    return response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_data = get_json(url)\n",
    "response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get address, latitude, longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lat_lng(url):\n",
    "    response = get_json(url)\n",
    "    result = response['results'][0]\n",
    "    formatted_address = result['formatted_address']\n",
    "    lat = result['geometry']['location']['lat']\n",
    "    lng = result['geometry']['location']['lng']\n",
    "    return formatted_address, lat, lng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_lat_lng(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = 'London Business School'\n",
    "url = 'https://maps.googleapis.com/maps/api/geocode/json?address={}'.format(address)\n",
    "get_lat_lng(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get list of addresses with lat, lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lat_lng_list(url):\n",
    "    response = get_json(url)\n",
    "    result_list = []\n",
    "    for result in response['results']:\n",
    "        formatted_address = result['formatted_address']\n",
    "        lat = result['geometry']['location']['lat']\n",
    "        lng = result['geometry']['location']['lng']\n",
    "        result_list.append((formatted_address, lat, lng))\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = 'Baker Street'\n",
    "url = 'https://maps.googleapis.com/maps/api/geocode/json?address={}'.format(address)\n",
    "get_lat_lng_list(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging in to a web server, e.g. wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store your credentials in a encrypted/protected file (line1 = name, line2 = pwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../credentials.txt') as f:\n",
    "    contents = f.read().split('\\n')\n",
    "    username = contents[0]\n",
    "    password = contents[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct object that contains requested login data\n",
    "Inspect the login-form in your browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>get the value of the login token</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_login_token(response):\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    token = soup.find('input', {'name': \"wpLoginToken\"}).get('value')\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    'wpName': username,\n",
    "    'wpPassword': password,\n",
    "    'wploginattempt': 'Log in',\n",
    "    'wpEditToken': '+\\\\',\n",
    "    'title': 'Special:UserLogin',\n",
    "    'authAction': 'login',\n",
    "    'force': '',\n",
    "    'wpForceHttps': '1',\n",
    "    'wpFromhttp': '1',\n",
    "    'wpLoginToken': 'get_login_token(response)'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Setup a session, login, and get data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with requests.session() as s:\n",
    "    response = s.get('https://en.wikipedia.org/w/index.php?title=Special:UserLogin&returnto=Main+Page')\n",
    "    \n",
    "    # Set login token\n",
    "    payload['wpLoginToken'] = get_login_token(response)\n",
    "    \n",
    "    # Send the login request\n",
    "    response_post = s.post('https://en.wikipedia.org/w/index.php?title=Special:UserLogin&action=submitlogin&type=login',\n",
    "                           data=payload)\n",
    "    \n",
    "    # Get another page and check if we’re still logged in\n",
    "    response = s.get('https://en.wikipedia.org/wiki/Special:Watchlist')\n",
    "    data = BeautifulSoup(response.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.find('div', class_='mw-changeslist').get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case example - 'Sgraping'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.jancisrobinson.com'\n",
    "red_ = '/learn/grape-varieties/red/'\n",
    "white_ = '/learn/grape-varieties/white/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hrefs(url, tag, class_, keywords=''):\n",
    "    try:\n",
    "        results_page = result_page(url, keywords)\n",
    "        href_list = results_page.find_all(tag, class_=class_)[0].find_all('a')\n",
    "        return ['{}{}'.format(url_base, href.get('href')) for href in href_list]\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrefs_red = get_hrefs(url, 'ul', 'info-table', red_)\n",
    "hrefs_white = get_hrefs(url, 'ul', 'info-table', white_)\n",
    "print(hrefs_white[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grape_text(url, tag, class_1, class_2, tag_1, color, keywords=''):\n",
    "    try:\n",
    "        results_page = result_page(url, keywords)\n",
    "        grape = results_page.find_all(tag, class_=class_1)[0].find_all(tag_1)[0].get_text()\n",
    "        content = results_page.find_all(tag, class_=class_2)[0].get_text()\n",
    "        return grape, color ,content\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sgrape all varieties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grape_varieties(url, tag, class_1, class_2, tag_1, color):\n",
    "    grape_var_list = []\n",
    "    for color in colors:\n",
    "        hrefs = get_hrefs(url, 'ul', 'info-table', '/learn/grape-varieties/' + color)\n",
    "        for href in hrefs_red[:10]:\n",
    "            grape_text_list.append(get_grape_text(href, tag, class_1, class_2, tag_1, color))\n",
    "    return grape_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url, tag, class_1, class_2, tag_1, colors = 'https://www.jancisrobinson.com', 'div', 'learn-header', 'row', 'h1', ['red', 'white']\n",
    "get_grape_varieties(href, tag, class_1, class_2, tag_1, colors);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grapes = pd.DataFrame(grape_text_list)\n",
    "df_grapes.columns = ['Grape', 'Color', 'Description']\n",
    "df_grapes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean - remove excessive spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grapes['Grape'] = [str(x.strip()) for x in df_grapes['Grape']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to UTF (English alfabeth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install unidecode\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode to english, removing special chars\n",
    "df_grapes['Grape_utf'] = [str(unidecode.unidecode(x).strip()) for x in df_grapes['Grape']]\n",
    "df_grapes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the grapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df_grapes\n",
    "df_grapes.to_csv('grape_descr.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import praw\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create credentials file:\n",
    "# !touch reddit_credentials.txt\n",
    "# !echo \"OUR_CLIENT_ID\\nOUR_SECRET\" > reddit_credentials.txt\n",
    "# !chmod 400 reddit_credentials.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reddit_credentials.txt') as f:\n",
    "    contents = f.read().split('\\n')\n",
    "    OUR_CLIENT_ID = contents[0]\n",
    "    OUR_SECRET = contents[1]\n",
    "APP = 'reddit_test_app/1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id=OUR_CLIENT_ID,\n",
    "    client_secret=OUR_SECRET,\n",
    "    grant_type='client_credentials',\n",
    "    user_agent=APP)\n",
    "subs = reddit.subreddit('Python').top(limit=10)\n",
    "pprint.pprint([(s.score, s.title) for s in subs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reddit():\n",
    "    return praw.Reddit(\n",
    "        client_id=OUR_CLIENT_ID,\n",
    "        client_secret=OUR_SECRET,\n",
    "        grant_type='client_credentials',\n",
    "        user_agent=APP\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top(subreddit_name):\n",
    "    today = datetime.now().strftime(r'%Y-%m-%d')\n",
    "    dirname = os.path.join('news-%s' % today, subreddit_name)\n",
    "    os.makedirs(dirname, exist_ok=True)\n",
    "\n",
    "    # Get top 50 submissions from reddit\n",
    "    reddit = get_reddit()\n",
    "    top_subs = reddit.subreddit(subreddit_name).top(limit=50)\n",
    "\n",
    "    # Remove those submissions that belongs to reddit\n",
    "    subs = [sub for sub in top_subs if not sub.domain.startswith('self.')]\n",
    "\n",
    "    count = 10\n",
    "    while subs and count > 0:\n",
    "        sub = subs.pop(0)\n",
    "        article = get_article(sub.url)\n",
    "        if article:\n",
    "            text = '\\n\\n'.join(article['content'])\n",
    "            filename = re.sub(r'\\W+', '_', article['title']) + '.md'\n",
    "            open(os.path.join(dirname, filename), 'w').write(text)\n",
    "            count -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article(url):\n",
    "    print('  - Retrieving %s' % url)\n",
    "    try:\n",
    "        res = requests.get(url)\n",
    "        if (res.status_code == 200 and 'content-type' in res.headers and\n",
    "                res.headers.get('content-type').startswith('text/html')):\n",
    "            article = parse_article(res.text)\n",
    "            print('      => done, title = \"%s\"' % article['title'])\n",
    "            return article\n",
    "        else:\n",
    "            print('      x fail or not html')\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "    # find the article title\n",
    "    h1 = soup.body.find('h1')\n",
    "\n",
    "    # find the common parent for <h1> and all <p>s.\n",
    "    root = h1\n",
    "    while root.name != 'body' and len(root.find_all('p')) < 5:\n",
    "        root = root.parent\n",
    "\n",
    "    if len(root.find_all('p')) < 5:\n",
    "        return None\n",
    "\n",
    "    # find all the content elements.\n",
    "    ps = root.find_all(['h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre'])\n",
    "    ps.insert(0, h1)\n",
    "    content = [tag2md(p) for p in ps]\n",
    "\n",
    "    return {'title': h1.text, 'content': content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag2md(tag):\n",
    "    if tag.name == 'p':\n",
    "        return tag.text\n",
    "    elif tag.name == 'h1':\n",
    "        return f'{tag.text}\\n{\"=\" * len(tag.text)}'\n",
    "    elif tag.name == 'h2':\n",
    "        return f'{tag.text}\\n{\"-\" * len(tag.text)}'\n",
    "    elif tag.name in ['h3', 'h4', 'h5', 'h6']:\n",
    "        return f'{\"#\" * int(tag.name[1:])} {tag.text}'\n",
    "    elif tag.name == 'pre':\n",
    "        return f'```\\n{tag.text}\\n```'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in subs:\n",
    "  res = requests.get(sub.url)\n",
    "  if (res.status_code == 200 and 'content-type' in res.headers and\n",
    "      res.headers.get('content-type').startswith('text/html')):\n",
    "    html = res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "subreddits = ['javascript', 'Python', 'news']\n",
    "for sr in subreddits:\n",
    "    print('Scraping /r/%s...' % sr)\n",
    "    get_top(sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beer\n",
    "http://blog.kaggle.com/2017/01/31/scraping-for-craft-beers-a-dataset-creation-tutorial/?utm_medium=email&utm_source=intercom&utm_campaign=new+user+onboarding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    " \n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determines if a table_row is a beer entry\n",
    "def is_beer_entry(table_row):\n",
    "    row_cells = table_row.findAll(\"td\")\n",
    "    beer_id = get_beer_id(row_cells[0].text)\n",
    "    return ( len(row_cells) == 8 and beer_id )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the beer entry numerical identifier from the \"Entry\" column.\n",
    "def get_beer_id(cell_value):\n",
    "    r = re.match(\"^(\\d{1,4})\\.$\", cell_value)\n",
    "    if r and len(r.groups()) == 1:\n",
    "        beer_id = r.group(1)\n",
    "        return int(beer_id)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_beers(html_soup):\n",
    "    beers = []\n",
    "    all_rows_in_html_page = html_soup.findAll(\"tr\")\n",
    "    for table_row in all_rows_in_html_page:\n",
    "        if is_beer_entry(table_row):\n",
    "            row_cells = table_row.findAll(\"td\")\n",
    "            beer_entry = {\n",
    "                \"id\": get_beer_id(row_cells[0].text),\n",
    "                \"name\": row_cells[1].text,\n",
    "                \"brewery_name\": row_cells[2].text,\n",
    "                \"brewery_location\": row_cells[3].text,\n",
    "                \"style\": row_cells[4].text,\n",
    "                \"size\": row_cells[5].text,\n",
    "                \"abv\": row_cells[6].text,    \n",
    "                \"ibu\": row_cells[7].text\n",
    "            }\n",
    "            beers.append(beer_entry)\n",
    "    return beers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://craftcans.com/db.php?search=all&sort=beerid&ord=desc&view=text\"\n",
    "html = urlopen(url)\n",
    "html_soup = BeautifulSoup(html, 'html.parser')\n",
    "beers_list = get_all_beers(html_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(beers_list)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breweries = df[[\"brewery_location\", \"brewery_name\"]]\n",
    "breweries = breweries.drop_duplicates().reset_index(drop=True)\n",
    "breweries[\"id\"] = breweries.index\n",
    "breweries.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beers = pd.merge(df,\n",
    "                 breweries,\n",
    "                 left_on=[\"brewery_name\", \"brewery_location\"],\n",
    "                 right_on=[\"brewery_name\", \"brewery_location\"],\n",
    "                 sort=True,\n",
    "                 suffixes=('_beer', '_brewery'))\n",
    "beers = beers[[\"abv\", \"ibu\", \"id_beer\",\n",
    "               \"name\", \"size\", \"style\", \"id_brewery\"]]\n",
    "beers_columns_rename = {\n",
    "    \"id_beer\": \"id\",\n",
    "    \"id_brewery\": \"brewery_id\"\n",
    "}\n",
    "beers.rename(inplace=True, columns=beers_columns_rename)\n",
    "beers.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breweries[\"city\"] = breweries[\"brewery_location\"].apply(\n",
    "    lambda location: location.split(\",\")[0])\n",
    "breweries[\"state\"] = breweries[\"brewery_location\"].apply(\n",
    "    lambda location: location.split(\",\")[1])\n",
    "breweries = breweries[[\"brewery_name\", \"city\", \"state\"]]\n",
    "breweries.rename(inplace=True, columns={\"brewery_name\": \"name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_pct_to_float(value):\n",
    "    stripped = str(value).strip('%')\n",
    "    try:\n",
    "        return float(stripped)/100\n",
    "    except ValueError:    \n",
    "        return None\n",
    " \n",
    "beers[\"abv\"] = beers[\"abv\"].apply(string_pct_to_float)\n",
    " \n",
    "def string_to_int(value):\n",
    "    try:\n",
    "        return int(value)\n",
    "    except ValueError:  \n",
    "        return None\n",
    " \n",
    "beers[\"ibu\"] = beers[\"ibu\"].apply(string_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for possible_value in set(beers[\"size\"].tolist()):\n",
    "    print(possible_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ounces(value):\n",
    "    stripped = value.strip(\"oz\")\n",
    "    match = re.match(\"(\\d{1,2}\\.*\\d*)\", value)\n",
    "    if match:\n",
    "        return float(match.group(0))\n",
    "    else:\n",
    "        return None\n",
    " \n",
    "beers[\"ounces\"] = beers[\"size\"].apply(extract_ounces)    \n",
    "del beers[\"size\"]\n",
    "beers.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTitle(url):\n",
    "    try:\n",
    "        html = urlopen(url)\n",
    "    except HTTPError as e:\n",
    "        print(e)\n",
    "        return None\n",
    "    try:\n",
    "        bsObj = BeautifulSoup(html.read(), 'html.parser')  # or 'lxml'\n",
    "        title = bsObj.body.h1\n",
    "    except AttributeError as e:\n",
    "        return None\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = getTitle(\"http://www.pythonscraping.com/exercises/exercise1.html\")\n",
    "if title == None:\n",
    "    print(\"Title could not be found\")\n",
    "else:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Select by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen(\"http://www.pythonscraping.com/pages/warandpeace.html\")\n",
    "bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "nameList = bsObj.findAll(\"span\", {\"class\": \"green\"})\n",
    "\n",
    "for name in nameList:\n",
    "    print(name.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Select by Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen(\"http://www.pythonscraping.com/pages/warandpeace.html\")\n",
    "bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "allText = bsObj.findAll(id=\"text\")\n",
    "print(allText[0].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find descendants(children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen(\"http://www.pythonscraping.com/pages/page3.html\")\n",
    "bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "for child in bsObj.find(\"table\",{\"id\": \"giftList\"}).children:\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find siblings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sibling in bsObj.find(\"table\",{\"id\": \"giftList\"}).tr.next_siblings:\n",
    "    print(sibling) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen(\"http://www.pythonscraping.com/pages/page3.html\")\n",
    "bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "print(bsObj.find(\"img\",{\"src\":\"../img/gifts/img1.jpg\"}).parent.previous_sibling.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen(\"http://www.pythonscraping.com/pages/page3.html\")\n",
    "bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "images = bsObj.findAll(\"img\", {\"src\":re.compile(\"\\.\\.\\/img\\/gifts/img.*\\.jpg\")})\n",
    "for image in images: \n",
    "    print(image[\"src\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lambda exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen(\"http://www.pythonscraping.com/pages/page2.html\")\n",
    "bsObj = BeautifulSoup(html, \"html.parser\")\n",
    "tags = bsObj.findAll(lambda tag: len(tag.attrs) == 2)\n",
    "for tag in tags:\n",
    "\tprint(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
