{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscraping Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping rules\n",
    "- You should check a site's terms and conditions before you scrape them. It's their data and they likely have some rules to govern it.\n",
    "- Be nice - A computer will send web requests much quicker than a user can. Make sure you space out your requests a bit so that you don't hammer the site's server.\n",
    "- Scrapers break - Sites change their layout all the time. If that happens, be prepared to rewrite your code.\n",
    "- Web pages are inconsistent - There's sometimes some manual clean up that has to happen even after you've gotten your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import necessary modules</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import praw\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create credentials file:\n",
    "# !touch reddit_credentials.txt\n",
    "# !echo \"OUR_CLIENT_ID\\nOUR_SECRET\" > reddit_credentials.txt\n",
    "# !chmod 400 reddit_credentials.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../_credentials/reddit_credentials.txt') as f:\n",
    "    contents = f.read().split('\\n')\n",
    "    OUR_CLIENT_ID = contents[0]\n",
    "    OUR_SECRET = contents[1]\n",
    "APP = 'reddit_test_app/1.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id=OUR_CLIENT_ID, client_secret=OUR_SECRET,\n",
    "                     grant_type='client_credentials', user_agent=APP)\n",
    "subs = reddit.subreddit('Python').top(limit=10)\n",
    "pprint.pprint([(s.score, s.title) for s in subs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reddit():\n",
    "    return praw.Reddit(client_id=OUR_CLIENT_ID, client_secret=OUR_SECRET,\n",
    "                       grant_type='client_credentials', user_agent=APP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top(subreddit_name, top_n=50):\n",
    "    \n",
    "    today = datetime.now().strftime(r'%Y-%m-%d')\n",
    "    dirname = os.path.join('../_data/news-{} ({})'.format(today, subreddit_name))\n",
    "    os.makedirs(dirname, exist_ok=True)\n",
    "\n",
    "    # Get top n submissions from reddit\n",
    "    reddit = get_reddit()\n",
    "    top_subs = reddit.subreddit(subreddit_name).top(limit=top_n)\n",
    "\n",
    "    # Remove those submissions that belongs to reddit\n",
    "    subs = [sub for sub in top_subs if not sub.domain.startswith('self.')]\n",
    "\n",
    "    count = 10\n",
    "    while subs and count > 0:\n",
    "        sub = subs.pop(0)\n",
    "        article = get_article(sub.url)\n",
    "        if article:\n",
    "            text = '\\n\\n'.join(article['content'])\n",
    "            filename = re.sub(r'\\W+', '_', article['title']) + '.md'\n",
    "            open(os.path.join(dirname, filename), 'w').write(text)\n",
    "            count -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article(url):\n",
    "    print('  - Retrieving {}'.format(url))\n",
    "    try:\n",
    "        res = requests.get(url)\n",
    "        if (res.status_code == 200 and 'content-type' in res.headers and\n",
    "                res.headers.get('content-type').startswith('text/html')):\n",
    "            article = parse_article(res.text)\n",
    "            print('      => done, title = \"{}\"'.format(article['title']))\n",
    "            return article\n",
    "        else:\n",
    "            print('      x fail or not html')\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "\n",
    "    # find the article title\n",
    "    h1 = soup.body.find('h1')\n",
    "\n",
    "    # find the common parent for <h1> and all <p>s.\n",
    "    root = h1\n",
    "    while root.name != 'body' and len(root.find_all('p')) < 5:\n",
    "        root = root.parent\n",
    "\n",
    "    if len(root.find_all('p')) < 5:\n",
    "        return None\n",
    "\n",
    "    # find all the content elements.\n",
    "    ps = root.find_all(['h2', 'h3', 'h4', 'h5', 'h6', 'p', 'pre'])\n",
    "    ps.insert(0, h1)\n",
    "    content = [tag2md(p) for p in ps]\n",
    "\n",
    "    return {'title': h1.text, 'content': content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag2md(tag):\n",
    "    if tag.name == 'p':\n",
    "        return tag.text\n",
    "    elif tag.name == 'h1':\n",
    "        return f'{tag.text}\\n{\"=\" * len(tag.text)}'\n",
    "    elif tag.name == 'h2':\n",
    "        return f'{tag.text}\\n{\"-\" * len(tag.text)}'\n",
    "    elif tag.name in ['h3', 'h4', 'h5', 'h6']:\n",
    "        return f'{\"#\" * int(tag.name[1:])} {tag.text}'\n",
    "    elif tag.name == 'pre':\n",
    "        return f'```\\n{tag.text}\\n```'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in subs:\n",
    "    res = requests.get(sub.url)\n",
    "    if (res.status_code == 200 and 'content-type' in res.headers and\n",
    "        res.headers.get('content-type').startswith('text/html')):\n",
    "        html = res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main\n",
    "subreddits = ['javascript', 'Python', 'news']\n",
    "for sr in subreddits:\n",
    "    print('\\nScraping: {}...'.format(sr))\n",
    "    get_top(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
