{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate expected misclassification rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = {'retina',}\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model to be used for generating data with two classes:\n",
    "<BR>\n",
    "$y_i = 0,\\quad x_i \\sim N_{10}(0,  I_{10})$\n",
    "\n",
    "$y_i = 1,\\quad x_i \\sim N_{10}(\\mu,  I_{10})\\,$ with $\\mu = (1,1,1,1,1,0,0,0,0,0)$\n",
    "\n",
    "$x_i \\in \\mathbb{R^{10}}$ normally distributed\n",
    "\n",
    "$y_i$ equally divided between the two classes (balanced dataset)\n",
    "<BR>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Gaussian Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu1 = np.repeat(0, 10)\n",
    "mu2 = np.repeat([0, 1], 5)\n",
    "sigma = np.identity(10)\n",
    "X = multivariate_normal(mean=mu1, cov=sigma).rvs(5)\n",
    "mu\n",
    "sigma\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_clf_error(clf, train_sample_n=100, test_sample_n=2000):\n",
    "    \n",
    "    # Generate training sample and train classifier\n",
    "    X_train_0 = multivariate_normal(mean=np.repeat(0, 10), cov=np.identity(10)).rvs(train_sample_n//2)\n",
    "    X_train_1 = multivariate_normal(mean=np.repeat([0, 1], 5), cov=np.identity(10)).rvs(train_sample_n//2)\n",
    "    \n",
    "    X_train = np.r_[X_train_0, X_train_1]\n",
    "    y_train = np.repeat([0, 1], train_sample_n//2)\n",
    "        \n",
    "    clf.fit(X_train, y_train)\n",
    "        \n",
    "    # Generate large set of test data and return error rate of classifier\n",
    "    X_test_0 = multivariate_normal(mean=np.repeat(0, 10), cov=np.identity(10)).rvs(test_sample_n//2)\n",
    "    X_test_1 = multivariate_normal(mean=np.repeat([0, 1], 5), cov=np.identity(10)).rvs(test_sample_n//2)\n",
    "        \n",
    "    X_test = np.r_[X_test_0, X_test_1]\n",
    "    y_test = np.repeat([0, 1], test_sample_n//2)\n",
    "        \n",
    "    return 1 - clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeats = 1000\n",
    "svm_radial = [simulate_clf_error(SVC(kernel='rbf')) for i in np.arange(repeats)]\n",
    "svm_linear = [simulate_clf_error(SVC(kernel='linear')) for i in np.arange(repeats)]\n",
    "log_regr = [simulate_clf_error(LogisticRegression(C=100)) for i in np.arange(repeats)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SVM - radial kernel: mean: {} sd: {}'.format(np.mean(svm_radial).round(3), np.var(svm_radial)**.5))\n",
    "print('SVM - linear kernel: mean: {} sd: {}'.format(np.mean(svm_linear).round(3), np.var(svm_linear)**.5))\n",
    "print('Logistic regression: mean: {} sd: {}'.format(np.mean(log_regr).round(3), np.var(log_regr)**.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(svm_radial, 'g', alpha=0.4, label='SVM Radial')\n",
    "plt.plot(svm_linear, 'r', alpha=0.4, label='SVM Linear')\n",
    "plt.plot(log_regr, 'b', alpha=0.4, label='Logistic Regression')\n",
    "\n",
    "plt.hlines(np.mean(np.c_[svm_radial, svm_linear, log_regr], axis=0), 0, repeats, colors=['g', 'r', 'b'])\n",
    "plt.xlabel('Simulation')\n",
    "plt.ylabel('Error rate')\n",
    "plt.title('Simulation: expected misclassification rates')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, sharey=True, figsize=(18,5))\n",
    "_ = ax1.plot(svm_radial, 'g', alpha=0.4, label='SVM Radial')\n",
    "_ = ax1.hlines(np.mean(svm_radial), 0, repeats, colors='g')\n",
    "_ = ax1.set_ylabel('Error rate')\n",
    "\n",
    "_ = ax2.plot(svm_linear, 'r', alpha=0.4, label='SVM Linear')\n",
    "_ = ax2.hlines(np.mean(svm_linear), 0, repeats, colors='r')\n",
    "\n",
    "_ = ax3.plot(log_regr, 'b', alpha=0.4, label='Logistic Regression')\n",
    "_ = ax3.hlines(np.mean(log_regr), 0, repeats, colors='b');\n",
    "\n",
    "_ = fig.suptitle('Simulation: expected misclassification rates', fontsize=16)\n",
    "_ = fig.subplots_adjust(wspace=0.02)\n",
    "\n",
    "for ax in fig.axes:\n",
    "    _ = ax.set_xlabel('{} simulations'.format(repeats))\n",
    "    _ = ax.tick_params(labelbottom='off')\n",
    "    _ = ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
