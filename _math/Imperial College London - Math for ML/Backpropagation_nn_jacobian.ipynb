{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a neural network to draw a curve.  \n",
    "The curve takes one input variable, the amount travelled along the curve from 0 to 1, and returns 2 outputs, the 2D coordinates of the position of points on the curve.\n",
    "\n",
    "To help capture the complexity of the curve, we shall use two hidden layers in our network with 6 and 7 neurons respectively.\n",
    "\n",
    "![Neural network with 2 hidden layers. There is 1 nodes in the zeroth layer, 6 in the first, 7 in the second, and 2 in the third.](../../_data/bigNet.png \"The structure of the network we will consider in this assignment.\")\n",
    "\n",
    "Complete functions that calculate the Jacobian of the cost function, with respect to the weights and biases of the network. The code will form part of a stochastic steepest descent algorithm that will train your network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"./BackpropModule.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward\n",
    "\n",
    "The following cells define functions to set up our neural network:\n",
    "  - an activation function, $\\sigma(z)$\n",
    "  - it's derivative, $\\sigma'(z)$\n",
    "  - a function to initialise weights and biases\n",
    "  - a function that calculates each activation of the network using feed-forward\n",
    "\n",
    "__Feed-forward equations__:  \n",
    "$$ \\mathbf{a}^{(n)} = \\sigma(\\mathbf{z}^{(n)}) $$\n",
    "$$ \\mathbf{z}^{(n)} = \\mathbf{W}^{(n)}\\mathbf{a}^{(n-1)} + \\mathbf{b}^{(n)} $$\n",
    "\n",
    "In this worksheet we will use the __sigmoid - logistic function__ as our activation function:  \n",
    "$$ \\sigma(\\mathbf{z}) = \\frac{1}{1 + \\exp(-\\mathbf{z})} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices in Python\n",
    "Matrices can be multiplied together in two ways.\n",
    "\n",
    "__Element wise__: when two matrices have the same dimensions, matrix elements in the same position in each matrix are multiplied together\n",
    "```python\n",
    "A = B * C\n",
    "```\n",
    "\n",
    "__Matrix multiplication__: when the number of columns in the first matrix is the same as the number of rows in the second.\n",
    "```python\n",
    "A = B @ C\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid ativation function and its derivative.\n",
    "sigma = lambda z : 1 / (1 + np.exp(-z))\n",
    "d_sigma = lambda z : np.cosh(z/2)**(-2) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function initialises the network with it's structure, it also resets any training already done.\n",
    "# 2 hidden layers; 6 and 7 units/neurons\n",
    "def reset_network (n1=6, n2=7, random=np.random) :\n",
    "    global W1, W2, W3, b1, b2, b3\n",
    "    W1 = random.randn(n1, 1) / 2\n",
    "    W2 = random.randn(n2, n1) / 2\n",
    "    W3 = random.randn(2, n2) / 2\n",
    "    b1 = random.randn(n1, 1) / 2\n",
    "    b2 = random.randn(n2, 1) / 2\n",
    "    b3 = random.randn(2, 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function feeds forward each activation to the next layer. It returns all weighted sums and activations.\n",
    "def network_function(a0) :\n",
    "    z1 = W1 @ a0 + b1\n",
    "    a1 = sigma(z1)\n",
    "    \n",
    "    z2 = W2 @ a1 + b2\n",
    "    a2 = sigma(z2)\n",
    "    \n",
    "    z3 = W3 @ a2 + b3\n",
    "    a3 = sigma(z3)\n",
    "    \n",
    "    return a0, z1, a1, z2, a2, z3, a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the cost function of a neural network with respect to a training set.\n",
    "# Mean Squared Distance between a3 and y\n",
    "def cost(x, y) :\n",
    "    return np.linalg.norm(network_function(x)[-1] - y)**2 / x.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "In the next cells, you will be asked to complete functions for the Jacobian of the cost function with respect to the weights and biases.\n",
    "We will start with layer 3, which is the easiest, and work backwards through the layers.\n",
    "\n",
    "We'll define our Jacobians as,\n",
    "$$ \\mathbf{J}_{\\mathbf{W}^{(3)}} = \\frac{\\partial C}{\\partial \\mathbf{W}^{(3)}} $$\n",
    "$$ \\mathbf{J}_{\\mathbf{b}^{(3)}} = \\frac{\\partial C}{\\partial \\mathbf{b}^{(3)}} $$\n",
    "etc., where $C$ is the average cost function over the training set. i.e.,\n",
    "$$ C = \\frac{1}{N}\\sum_k C_k $$\n",
    "You calculated the following in the practice quizzes,\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{W}^{(3)}} =\n",
    "   \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}}\n",
    "   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{z}^{(3)}}\n",
    "   \\frac{\\partial \\mathbf{z}^{(3)}}{\\partial \\mathbf{W}^{(3)}}\n",
    "   ,$$\n",
    "for the weight, and similarly for the bias,\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{b}^{(3)}} =\n",
    "   \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}}\n",
    "   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{z}^{(3)}}\n",
    "   \\frac{\\partial \\mathbf{z}^{(3)}}{\\partial \\mathbf{b}^{(3)}}\n",
    "   .$$\n",
    "With the partial derivatives taking the form,\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}} = 2(\\mathbf{a}^{(3)} - \\mathbf{y}) $$\n",
    "$$ \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{z}^{(3)}} = \\sigma'({z}^{(3)})$$\n",
    "$$ \\frac{\\partial \\mathbf{z}^{(3)}}{\\partial \\mathbf{W}^{(3)}} = \\mathbf{a}^{(2)}$$\n",
    "$$ \\frac{\\partial \\mathbf{z}^{(3)}}{\\partial \\mathbf{b}^{(3)}} = 1$$\n",
    "\n",
    "Function 'J_W3' is ($\\mathbf{J}_{\\mathbf{W}^{(3)}}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jacobian for the third layer weights. \n",
    "def J_W3 (x, y) :\n",
    "    # Get all activations and weighted sums at each layer of the network\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    \n",
    "    # dC/da3 = 2*(a3 - y)\n",
    "    J = 2 * (a3 - y)\n",
    "    \n",
    "    # dC/dz3 = dC/da3 * da3/dz3 (the derivative of sigma, evaluated at z3).\n",
    "    J = J * d_sigma(z3)\n",
    "    \n",
    "    # dz3/dW3 = a2\n",
    "    # dC/dW3 = dz3/dW3 • dC/dz3 = dz3/dW3 • a2.T\n",
    "    # take the mean over all examples and divide by the number of training examples\n",
    "    J = J @ a2.T / x.size\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jacobian for the third layer bias.\n",
    "def J_b3 (x, y) :\n",
    "    # Get all activations and weighted sums at each layer of the network\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    \n",
    "    # dC/dz3 = dC/da3 * da3/dz3\n",
    "    J = 2 * (a3 - y)\n",
    "    J = J * d_sigma(z3)\n",
    "    \n",
    "    # We don't need to multiply: dz3/db3 = 1 ## Note: derivative of z wrt. b = 1\n",
    "    # We still need to sum over all training examples however\n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.size\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll next do the Jacobian for the Layer 2. The partial derivatives for this are,\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{W}^{(2)}} =\n",
    "   \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}}\n",
    "   \\left(\n",
    "   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{a}^{(2)}}\n",
    "   \\right)\n",
    "   \\frac{\\partial \\mathbf{a}^{(2)}}{\\partial \\mathbf{z}^{(2)}}\n",
    "   \\frac{\\partial \\mathbf{z}^{(2)}}{\\partial \\mathbf{W}^{(2)}}\n",
    "   ,$$\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{b}^{(2)}} =\n",
    "   \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}}\n",
    "   \\left(\n",
    "   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{a}^{(2)}}\n",
    "   \\right)\n",
    "   \\frac{\\partial \\mathbf{a}^{(2)}}{\\partial \\mathbf{z}^{(2)}}\n",
    "   \\frac{\\partial \\mathbf{z}^{(2)}}{\\partial \\mathbf{b}^{(2)}}\n",
    "   .$$\n",
    "This is very similar to the previous layer, with two exceptions:\n",
    "* There is a new partial derivative, in parentheses, $\\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{a}^{(2)}}$\n",
    "* The terms after the parentheses are now one layer lower.\n",
    "\n",
    "Recall the new partial derivative takes the following form,\n",
    "$$ \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{a}^{(2)}} =\n",
    "   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{z}^{(3)}}\n",
    "   \\frac{\\partial \\mathbf{z}^{(3)}}{\\partial \\mathbf{a}^{(2)}} =\n",
    "   \\sigma'(\\mathbf{z}^{(3)})\n",
    "   \\mathbf{W}^{(3)}\n",
    "$$\n",
    "\n",
    "To show how this changes things, we will implement the Jacobian for the weight again and ask you to implement it for the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J_W2 (x, y) :\n",
    "    #The first two lines are identical to in J_W3.\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)    \n",
    "    \n",
    "    # the next two lines implement da3/da2, first σ' and then W3.\n",
    "    J = 2 * (a3 - y)\n",
    "    J = J * d_sigma(z3) ## = dC/dz3\n",
    "    J = (J.T @ W3).T    ## = dC/dz3 • W3\n",
    "    \n",
    "    # then the final lines are the same as in J_W3 but with the layer number bumped down.\n",
    "    J = J * d_sigma(z2)\n",
    "    J = J @ a1.T / x.size\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J_b2 (x, y) :\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    \n",
    "    J = 2 * (a3 - y)\n",
    "    J = J * d_sigma(z3)\n",
    "    J = (J.T @ W3).T\n",
    "    \n",
    "    J = J * d_sigma(z2)\n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.size\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer 1 is very similar to Layer 2, but with an addition partial derivative term.\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{W}^{(1)}} =\n",
    "   \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}}\n",
    "   \\left(\n",
    "   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{a}^{(2)}}\n",
    "   \\frac{\\partial \\mathbf{a}^{(2)}}{\\partial \\mathbf{a}^{(1)}}\n",
    "   \\right)\n",
    "   \\frac{\\partial \\mathbf{a}^{(1)}}{\\partial \\mathbf{z}^{(1)}}\n",
    "   \\frac{\\partial \\mathbf{z}^{(1)}}{\\partial \\mathbf{W}^{(1)}}\n",
    "   ,$$\n",
    "$$ \\frac{\\partial C}{\\partial \\mathbf{b}^{(1)}} =\n",
    "   \\frac{\\partial C}{\\partial \\mathbf{a}^{(3)}}\n",
    "   \\left(\n",
    "   \\frac{\\partial \\mathbf{a}^{(3)}}{\\partial \\mathbf{a}^{(2)}}\n",
    "   \\frac{\\partial \\mathbf{a}^{(2)}}{\\partial \\mathbf{a}^{(1)}}\n",
    "   \\right)\n",
    "   \\frac{\\partial \\mathbf{a}^{(1)}}{\\partial \\mathbf{z}^{(1)}}\n",
    "   \\frac{\\partial \\mathbf{z}^{(1)}}{\\partial \\mathbf{b}^{(1)}}\n",
    "   .$$\n",
    "You should be able to adapt lines from the previous cells to complete **both** the weight and bias Jacobian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J_W1 (x, y) :\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    \n",
    "    J = 2 * (a3 - y)\n",
    "    J = J * d_sigma(z3)\n",
    "    J = (J.T @ W3).T\n",
    "    \n",
    "    J = J * d_sigma(z2)\n",
    "    J = (J.T @ W2).T\n",
    "    \n",
    "    J = J * d_sigma(z1)\n",
    "    J = J @ a0.T / x.size\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J_b1 (x, y) :\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    \n",
    "    J = 2 * (a3 - y)\n",
    "    J = J * d_sigma(z3)\n",
    "    J = (J.T @ W3).T\n",
    "    \n",
    "    J = J * d_sigma(z2)\n",
    "    J = (J.T @ W2).T\n",
    "    \n",
    "    J = J * d_sigma(z1)\n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.size\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we generate training data, and generate a network with randomly assigned weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = training_data()\n",
    "reset_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will iterate through a steepest descent algorithm using the Jacobians you have calculated.  \n",
    "The function will plot the training data (in green), and your neural network solutions in pink for each iteration, and orange for the last output.\n",
    "\n",
    "It takes about 50,000 iterations to train this network.\n",
    "We can split this up though - **10,000 iterations should take about a minute to run**.\n",
    "Run the line below as many times as you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(x, y, iterations=20000, aggression=7, noise=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish, you can change parameters of the steepest descent algorithm (We'll go into more details in future exercises), but you can change how many iterations are plotted, how agressive the step down the Jacobian is, and how much noise to add.\n",
    "\n",
    "You can also edit the parameters of the neural network, i.e. to give it different amounts of neurons in the hidden layers by calling,\n",
    "```python\n",
    "reset_network(n1, n2)\n",
    "```\n",
    "\n",
    "Play around with the parameters, and save your favourite result for the discussion prompt - *I ❤️ backpropagation*."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
