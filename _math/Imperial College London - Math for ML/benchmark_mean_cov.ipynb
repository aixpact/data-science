{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark naive mean covariance vs. numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "from sklearn.datasets import fetch_lfw_people, fetch_mldata, fetch_olivetti_faces\n",
    "import time\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to retrieve Olivetti faces dataset.\n",
    "\n",
    "When working with some datasets, before digging into further analysis, it is almost always\n",
    "useful to do a few things to understand your dataset. First of all, answer the following\n",
    "set of questions:\n",
    "\n",
    "1. What is the size of your dataset?\n",
    "2. What is the dimensionality of your data?\n",
    "\n",
    "The dataset we have are usually stored as 2D matrices, then it would be really important\n",
    "to know which dimension represents the dimension of the dataset, and which represents\n",
    "the data points in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (64, 64)\n",
    "# Load faces data\n",
    "dataset = fetch_olivetti_faces()\n",
    "faces = dataset.data\n",
    "\n",
    "print('Shape of the faces dataset: {}'.format(faces.shape))\n",
    "print('{} data points'.format(faces.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When your dataset are images, it's a really good idea to see what they look like.\n",
    "\n",
    "One very\n",
    "convenient tool in Jupyter is the `interact` widget, which we use to visualize the images (faces). For more information on how to use interact, have a look at the documentation [here](http://ipywidgets.readthedocs.io/en/stable/examples/Using%20Interact.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(n=(0, len(faces)-1))\n",
    "def display_faces(n=0):\n",
    "    plt.figure()\n",
    "    plt.imshow(faces[n].reshape((64, 64)), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean and Covariance of a Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__When you implement the functions for your assignment, make sure you read\n",
    "the docstring which dimension of your inputs corresponds to the number of data points and which \n",
    "corresponds to the dimension of the dataset.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_naive(X):\n",
    "    \"\"\"Compute the mean for a dataset by iterating over the dataset\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    X: (N, D) ndarray representing the dataset.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mean: (D, ) ndarray which is the mean of the dataset.\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    mean = np.zeros(D)\n",
    "    for d in range(D):\n",
    "        mean[d] = np.mean(X[:,d]) # EDIT THIS\n",
    "    return mean\n",
    "\n",
    "def cov_naive(X):\n",
    "    \"\"\"Compute the covariance for a dataset\n",
    "    Arguments\n",
    "    ---------\n",
    "    X: (N, D) ndarray representing the dataset.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    covariance: (D, D) ndarray which is the covariance matrix of the dataset.\n",
    "    \n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    covariance = np.zeros((D, D))\n",
    "    covariance = np.cov(X, rowvar=False)\n",
    "    for n in range(N):\n",
    "        X[n,:].T @ X[n,:] # fake fje    \n",
    "    return covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(X):\n",
    "    \"\"\"Compute the mean for a dataset\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    X: (N, D) ndarray representing the dataset.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mean: (D, ) ndarray which is the mean of the dataset.\n",
    "    \"\"\"\n",
    "    mean = np.mean(X, axis=0) # EDIT THIS\n",
    "    return mean\n",
    " \n",
    "# ===YOU SHOULD EDIT THIS FUNCTION===\n",
    "def cov(X):\n",
    "    \"\"\"Compute the covariance for a dataset\n",
    "    Arguments\n",
    "    ---------\n",
    "    X: (N, D) ndarray representing the dataset.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    covariance_matrix: (D, D) ndarray which is the covariance matrix of the dataset.\n",
    "    \n",
    "    \"\"\"\n",
    "    # It is possible to vectorize our code for computing the covariance, i.e. we do not need to explicitly\n",
    "    # iterate over the entire dataset as looping in Python tends to be slow\n",
    "    N, D = X.shape\n",
    "    covariance_matrix = np.cov(X, rowvar=False) # EDIT THIS\n",
    "    return covariance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `mean` function implemented, let's take a look at the _mean_ face of our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.mean(faces, axis=0).reshape((64, 64)), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put things into perspective, we can benchmark the two different implementation with the `%time` function\n",
    "in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have some huge data matrix, and we want to compute its mean\n",
    "X = np.random.randn(100000, 20)\n",
    "# Benchmarking time for computing mean\n",
    "%time mean_naive(X)\n",
    "%time mean(X)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking time for computing covariance\n",
    "%time cov_naive(X)\n",
    "%time cov(X)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can also see how running time increases as we increase the size of our dataset.\n",
    "In the following cell, we run `mean`, `mean_naive` and `cov`, `cov_naive` for many times on different sizes of\n",
    "the dataset and collect their running time. If you are less familiar with Python, you may want to spend\n",
    "some time understanding what the code does. __Understanding how your code scales with the size of your dataset (or dimensionality of the dataset) is crucial__ when you want to apply your algorithm to larger dataset. This is really important when we propose alternative methods a more efficient algorithms to solve the same problem. We will use these techniques again later in this course to analyze the running time of our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time(f, repeat=100):\n",
    "    \"\"\"A helper function to time the execution of a function.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    f: a function which we want to time it.\n",
    "    repeat: the number of times we want to execute `f`\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    the mean and standard deviation of the execution.\n",
    "    \"\"\"\n",
    "    times = []\n",
    "    for _ in range(repeat):\n",
    "        start = timeit.default_timer()\n",
    "        f()\n",
    "        stop = timeit.default_timer()\n",
    "        times.append(stop-start)\n",
    "    return np.mean(times), np.std(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_time = []\n",
    "slow_time = []\n",
    "\n",
    "for size in np.arange(100, 5000, step=100):\n",
    "    X = np.random.randn(size, 20)\n",
    "    f = lambda : mean(X)\n",
    "    mu, sigma = time(f)\n",
    "    fast_time.append((size, mu, sigma))\n",
    "    \n",
    "    f = lambda : mean_naive(X)\n",
    "    mu, sigma = time(f)\n",
    "    slow_time.append((size, mu, sigma))\n",
    "\n",
    "fast_time = np.array(fast_time)\n",
    "slow_time = np.array(slow_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.errorbar(fast_time[:,0], fast_time[:,1], fast_time[:,2], label='fast mean', linewidth=2)\n",
    "ax.errorbar(slow_time[:,0], slow_time[:,1], slow_time[:,2], label='naive mean', linewidth=2)\n",
    "ax.set_xlabel('size of dataset')\n",
    "ax.set_ylabel('running time')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## === FILL IN THIS, follow the approach we have above ===\n",
    "fast_time_cov = []\n",
    "slow_time_cov = []\n",
    "for size in np.arange(100, 5000, step=100):\n",
    "    X = np.random.randn(size, 20)\n",
    "    f = lambda : cov(X)    # EDIT THIS\n",
    "    mu, sigma = time(f)          # EDIT THIS\n",
    "    fast_time_cov.append((size, mu, sigma))\n",
    "    \n",
    "    f = lambda : cov_naive(X)          # EDIT THIS\n",
    "    mu, sigma = time(f)          # EDIT THIS\n",
    "    slow_time_cov.append((size, mu, sigma))\n",
    "\n",
    "fast_time_cov = np.array(fast_time_cov)\n",
    "slow_time_cov = np.array(slow_time_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.errorbar(fast_time_cov[:,0], fast_time_cov[:,1], fast_time_cov[:,2], label='fast covariance', linewidth=2)\n",
    "ax.errorbar(slow_time_cov[:,0], slow_time_cov[:,1], slow_time_cov[:,2], label='naive covariance', linewidth=2)\n",
    "ax.set_xlabel('size of dataset')\n",
    "ax.set_ylabel('running time')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "#ipympl\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from ipywidgets import interact\n",
    "MNIST = fetch_mldata('MNIST original', data_home='../../_data/MNIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(MNIST.data[MNIST.target==0].reshape(-1, 28, 28)[0], cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(x, y):\n",
    "    \"\"\"Compute distance between two vectors x, y using the dot product\"\"\"\n",
    "    x = np.array(x, dtype=np.float).ravel() # ravel() \"flattens\" the ndarray\n",
    "    y = np.array(y, dtype=np.float).ravel()\n",
    "    distance = np.sqrt((x-y).T @ (x-y))\n",
    "    return distance\n",
    "\n",
    "def angle(x, y):\n",
    "    \"\"\"Compute the angle between two vectors x, y using the dot product\"\"\"\n",
    "    angle = np.arccos(x.T @ y / (np.sqrt(x.T @ x) * np.sqrt(x.T @ x)))\n",
    "    return angle\n",
    "\n",
    "def pairwise_distance_matrix(X, Y):\n",
    "    \"\"\"Compute the pairwise distance between rows of X and rows of Y\n",
    "\n",
    "    Arguments\n",
    "    ----------\n",
    "    X: ndarray of size (N, D)\n",
    "    Y: ndarray of size (M, D)\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    D: matrix of shape (N, M), each entry D[i,j] is the distance between\n",
    "    X[i,:] and Y[j,:] using the dot product.\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    M, _ = Y.shape\n",
    "    distance_matrix = np.zeros((N, M), dtype=np.float) \n",
    "    for i in range(N):\n",
    "        for j in range(M):\n",
    "            distance_matrix[i, j] = distance(X[i,:], Y[j,:])\n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double looping (i,j) vs. np.ndenumerate() vs. pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "distances = []\n",
    "R = range(len(MNIST.data[:500]))  \n",
    "for i in R:\n",
    "    for j in R:\n",
    "        distances.append(distance(MNIST.data[i], MNIST.data[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_map(data, fn, N=0):\n",
    "    if N == 0: N = len(data)\n",
    "    Z = np.zeros(N*N).reshape(-1, N)\n",
    "    for (i,j), v in np.ndenumerate(Z):\n",
    "        Z[i,j] = fn(data[i], data[j])\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "Z = pairwise_map(MNIST.data, distance, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "pairwise_distances(MNIST.data[:500]).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z.round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = {}\n",
    "for n in np.unique(MNIST.target).astype(np.int):\n",
    "    means[n] = np.mean(MNIST.data[MNIST.target==n], axis=0)\n",
    "means[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_v = means.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.subtract.outer(mean_v, mean_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.subtract.outer(np.arange(10), np.arange(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = MNIST.data[:50]\n",
    "# (X - X).T @ (X - X)\n",
    "np.subtract.outer(X, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
